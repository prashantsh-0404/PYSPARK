{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0277324a-397e-49cd-810f-5a4d4b0eb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/26 09:05:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo\").config(\"spark.sql.inMemoryColumnarStorageCompression\", \"True\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d57354-dbbe-4b72-9842-8327d2a4f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config to disable self ambiguous join check\n",
    "spark.conf.set(\"pyspark.sql.analyzer.failAmbiguousSelfJoin\", False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6c1cc8-126a-47bb-8075-5237a0414f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache data in memory\n",
    "spark.conf.set(\"spark.sql.inMemoryColumnarStorageCompression\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c53f5c71-d197-41c4-9d2e-f6e6bec4b151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.analyzer.failAmbiguousSelfJoin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b029cc44-4a27-48c8-a00a-617ab4eb1c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark properties control most application parameters and can be set by using a SparkConf object, or through Java system properties.\\nEnvironment variables can be used to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node.\\nLogging can be configured through log4j2.properties.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark provides three locations to configure the system:\n",
    "'''\n",
    "Spark properties control most application parameters and can be set by using a SparkConf object, or through Java system properties.\n",
    "Environment variables can be used to set per-machine settings, such as the IP address, through the conf/spark-env.sh script on each node.\n",
    "Logging can be configured through log4j2.properties.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f25754-2377-430e-a8ea-ae75a0fd6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e294490-c863-420c-a8c3-226ffee87ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x112439690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setMaster(\"local\").setAppName(\"configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fdb7ca-8c3a-460d-8295-33bc3efc2a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get(\"spark.master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f874e36b-9cbf-46e0-8a5d-e9183ea4c76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'configurations'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get(\"spark.app.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bc6cb08-c876-427f-baa4-ffada60c5c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x112439690>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.sql.inMemoryColumnarStorageCompression\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe1c5a0a-aac5-4a87-8203-b59aef6cede5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.get(\"spark.sql.inMemoryColumnarStorageCompression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c85441e4-6db5-43d0-8214-ffcc0cdec21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x112439690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.sql.inMemoryColumnarStorage.batchsize\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b831921a-3639-4718-bd79-661c0c87eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x112439690>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.sql.broadcastTimeout\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45f93509-e363-4c53-9a47-b9e5d0cfc196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'300000ms'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.broadcastTimeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9fb5c0-8ade-4394-b892-8b3216f07f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.get(\"spark.sql.broadcastTimeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed4622-5a3b-417e-8409-01b3539f305e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
